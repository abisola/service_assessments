User Needs:
  1:
    criteria: Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for digital and assisted digital service design.
    prompts: |
      * What user research have you completed during the beta? (should be more than just a one off survey)
      * How many users did you speak to?
      * Have you done qualitative and quantitative research?
      * What have you learnt about your users that is specifically relevant to the design of the service?
      * How has your research being applied to the design of the service?
      * What are the user needs for your service? (Can you capture them in one sentence?)
      * Have you created user stories based on user needs?
      * What user research do you plan to do after the service is live?
      * Have you researched what the service should be called to best meet user needs, using user research and digital landscape analytics?
      * And discussed with GOV.UK to ensure the start page is optimised?
    evidence: |
      Service Manager able to:

        - explain cleanly what user research has been completed during the beta including qualitative usability testing
        - explain the user research methods, frequency of testing (normally with at least 5 users for each sprint), types of recruits and where they were recruited from, how the full service has been tested and in what environments, resources available and communication process into the service team to inform the design        
        - explain what has been learnt about users and user needs, including population demographics and digital proficiency profiles
        - give an example of a user story, the need it relates to and how that user need was determined
        - explain how the service was name was determined through user research and digital landscape analytics
  20:
    criteria: Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users.
    prompts: |
      * What did you learn from user research and usability testing during the beta?
      * How has this been incorporated into the service design?
      * What is your research plan for after the service is live?
      * Are the resources in place to do regular user research and usability testing after the service is live?
      * Do you have a testing environment in place for after the service is live?
      * Who in the team is doing user research and usability testing after the service is live?
      * How often are you doing user research and usability testing after the service is live?
      * How will the results feed into the design of the service?
      * What is the user research plan after the service goes live and are there resources for user research and usability testing?
    evidence: |
      Service Manager able to:

        - explain what they learnt during the beta and how this was incorporated into the service design
        - explain their research plan in detail for after the service is live
        - explain who is doing user research and usability testing and how it is being resourced once the service is live
        - explain their testing environment
        - explain how often they will be doing user research and usability testing once the service is live
        - explain how the results from user research and usability testing will be incorporated into the design of the service once the service is live

The Team:  
  2:
    criteria: Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility
    prompts: |
      * Can you talk us through the team that was in place during the beta and what the team will look like after the service is live?
      * Was the service manager empowered to make decisions during the beta and will this remain be the case after the service goes live?
      * Is the service manager the single responsible person with the power and knowledge to make decisions to improve the service day to day after the service is live?
      * Were there any gaps in the team during the beta and how are you addressing these for after the service goes live?
      * Do you have plans to sustain the team after the service goes live?
      * Is there an operating model in place for the live service?
      * Does the service manager understand the service fully?
      * How much help have you had from GDS and are you able to be self sufficient after the service goes live?
      * Are you using external people and if so, how are you approaching transferring skills and knowledge to the team after the service is live?
      * Is there a separation of key roles?

    evidence: |
      Service Manager able to:

        - clearly explain the structure of the team during the beta and after the service goes live (the following should be either in the team or available to the team depending on the scale of the service - service manager, product manager, delivery manager, tech lead, assisted digital lead, designer, user researcher, developers, content designer, technical architect, web ops, product analyst)
        - show they were empowered to make decisions during the beta and will continue to do so after the service is live
        - show there are no gaps in the team or explain how they will address any gaps
        - show that the team will be sustained to continuously improve the service after it goes live
        - show they fully understand the service
        - explain how the service will be self-sufficient after the service is live, where the service has received help from GDS
        - explain how they will transfer knowledge from external people to the team
        - show that there is a separation of key roles (i.e. the same person is not performing multiple roles within the service)
        - there is at least one user researcher working at least 3 days per week
  6:
    criteria: Build the service using the agile, iterative and user-centred methods set out in the manual
    prompts: |
      * Talk us through how you have worked in an agile way during the beta and how you will continue to do so after the service is live?
      * What tools and techniques have you used during the beta to enable this way of working?
      * How have you reviewed and iterated your processes during the beta?
      * How have you adapted your processes to be responsive and iterated them during the beta? 
      * How have you communicated within the team during the beta?
      * Can you give an example from the beta of how you have responded to user research and usability testing?

    evidence: |
      Service Manager able to:

        - clearly explain how the service has worked in an agile way during the beta and will continue to do so after the service is live, giving examples of using agile tools and techniques
        - explain how the service has reviewed and iterated their processes to be responsive during the beta
        - explain how the team has used agile tools and techniques to communicate within the team during the beta
        - give an example of how the service has responded to user research and usability testing during the beta

Security, Privacy, Tools and Standards:
  3:
    criteria: Evaluate what user data and information the service will be providing or storing, and address the security level, legal responsibilities, and risks associated with the service (consulting with experts where appropriate)
    prompts: |
      * Have you engaged with the right people?
      * Has a SIRO (Senior Information Risk Owner) been assigned and who are they?
      * Has a (IAO) Information Asset Owner been assigned?
      * Has an Accreditor been assigned to the project?
      * Have you set the risk appetite for the project?
      * What are you worried about?
      * Why is the architecture of the service the way it is?
      * What is the relationship between the service manager and the SIRO?
      * Have you done a risk assessment and who has signed off the risks?
      * Is the level of security appropriate (i.e. not too high or too low)?
      * Have you got SIRO and Accreditor approval to operate?
      * What are the major residual risks?
      * Has the service been subject to an assessment using RSDOPS?
      * What identity assurance needs were identified and how are they being met?  

    evidence: |
      Service Manager able to:

        - show that the security of the service is owned by the team under the service manager and that there has been proportionate external validation
        - explain who they have engaged with
        - confirm who the SIRO, Information Asset Owner and Accreditor are for the project
        - explain what the risk appetite is for the project
        - explain the architecture of the service and why they have chosen it
        - confirm they have done a risk assessment and explain who has signed off the risks
        - explain the level of security of and why it was chosen
        - confirm they have the SIRO and Accreditor's approval to operate
        - explain what the major residual risks are and why they are acceptable risks
        - confirm service assessed against RSDOPS to identify the risks and the extent to which the service needs to know that users are who they say they are
        - identified if the service needs to use identity assurance and if so what type/level.
  4:
    criteria: Evaluate the privacy risks to make sure that personal data collection requirements are appropriate
    prompts: |
      * What have you done to ensure compliance with the Data Protection Act?
      * What data goes where and why?
      * Who is the data controller for the service?
      * How have you ensured you follow cookie policy?
      * Has a Privacy Impact Assessment been carried out?
      * Has the SIRO signed off any risks?

    evidence: |
      Service Manager able to:

        - explain how the service complies with the Data Protection Act
        - explain data flows i.e. what data goes where and why
        - confirm who the data controller is for the service
        - explain how the service complies with cookie policy
        - confirm that a Privacy Impact Assessment has been carried out and that the SIRO has signed off any risks
  5:
    criteria: Evaluate what tools and systems will be used to build, host, operate and measure a service, and how to procure them
    prompts: |
      * How did you choose technology for the live service?
      * What tools are you using for the live service?
      * What options were considered and what factors led to the decision?
      * How are you avoiding lock-in?
      * Are the contracts in place suitably flexible and delivering value for money?
      * What procurement vehicles have you used and why?
      * What changes are you planning to make after the service is live?
      * What metrics are you collecting from the running system ?
      * How will you monitor and manage capacity after the service is live?
      * How will you respond to increases in demand after the service is live?
      * Do the operational team know what to do in the event of an incident on the service and have you tested this?

    evidence: |
      Service Manager able to:

        - explain what technology and tools the service has bought, built and will use for the live service and why they have chosen them
        - explain how they are avoiding lock in, how their contracts are suitably flexible and how they are delivering value for money
        - explain what procurement vehicles they have used and why
        - explain what changes they are planning to make after the service is live
        - explain what metrics they are collecting from the running system
        - explain how they will monitor and manage capacity after the service is live, and how they will respond to increases in demand
        - explain the process in the event of an incident on the service and how this has been tested
  15:
    criteria: Make all new source code open and reuseable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code)
    prompts: |
      * Do you have a policy in place for making new source code open and reuseable after the service is live?
      * What licences are you using to release code after the service is live?
      * Do you own the intellectual property?
      * Can I reuse your code in another department?
      * What have you open sourced for after the service is live?
      * For code that has not been published, why hasn't it?

    evidence: |
      Service Manager able to:

        - explain their policy for making all new source code open and reusable after the service is live
        - explain what licences they are using to release code after the service is live
        - confirm that the service owns the intellectual property or give a valid explanation why not
        - explain how other departments will be able to reuse their code after the service is live
        - show an example of something that is open sourced for the live service
        - explain which code has not been published, giving a valid reason why it hasn't
  16:
    criteria: Use open standards and common government platforms (eg identity assurance) where available
    prompts: |
      * What open standards are you using?
      * Are you complying with the open standards board's decisions?
      * Are you imposing vendor technology choices on users?
      * What standards are you using to reduce lock-in?
      * Are you intending to use the identity assurance platform when it is available?

    evidence: |
      Service Manager able to:

        - explain what open standards the service will use after the service has gone live
        - explain how they are complying with the open standards board's decisions
        - explain how they are avoiding imposing vendor technology decisions on users and reducing lock-in
        - explain whether they are intending to use the identity assurance platform for their service and if not, why not
  17:
    criteria: Be able to test the end-to-end service in an environment identical to that of the live version on all common browsers and devices. Use dummy accounts and a representative sample of users.
    prompts: |
      * How will you deploy in the live service and test before deploying?
      * Where do you do performance testing?
      * Do you have an environment for previewing new features?
      * Are you doing ongoing penetration testing?
      * What browsers and devices do you support and why?

    evidence: |
      Service Manager able to:

        - explain how they will deploy during the live service and test before deploying
        - explain how and where they will do performance testing for the live service
        - explain how they can preview new features before deploying them to the live service
        - explain their plan for ongoing penetration testing after the service is live
        - confirm that the service supports all browsers and devices identified in the service manual, and if not provide a valid reason why not
  25:
    criteria: Make a plan for the event of the service being taken temporarily offline.
    prompts: |
      * Do you have a disaster recovery plan in place for after the service is live and have you tested it?
      * Have you evaluated your suppliers disaster recovery capability?
      * Have you planned for the consequences of a forced shutdown after the service is live?
      * Have you planned for a Distributed Denial of Service attack and other malicious attacks after the service is live? Have you tested it?
      * Have you done sufficient load testing for after the service is live?
      * Do you have a line into GovCertUK for reporting in the event of an incident?

    evidence: |
      Service Manager able to:

        - explain their disaster recovery plan for the live service and how they have tested it
        - explain what their suppliers disaster recovery capability is where appropriate
        - explain how the service would deal with a forced shutdown of the live service
        - explain how they will deal with a DDoS attack and other malicious attacks on the live service
        - explain what load testing they have done for the live service
        - confirm they are aware of GovCertUK and are aware of how to report an incident

Improving the Service:
  14:
    criteria: Make sure that you have the capacity and technical flexibility to update and improve the service on a very frequent basis
    prompts: |
      * Talk us through your deployment process after the service is live?
      * How long will it take to make a change to the service after the service is live?
      * Who can make a change after the service is live?
      * Who can authorise making a change after the service is live?
      * How will you test a change before it is made after the service is live?
      * Will you require downtime for a release after the service is live? How long is the downtime?

    evidence: |
      Service Manager able to:

        - clearly explain their deployment process for after the service is live
        - explain how long it will take to make a change to the service after the service is live
        - explain who can make a change and who can authorise a change after the service is live
        - explain how they will test a change before it is made after the service is live
        - explain why the service requires downtime for a release and how long that downtime will be after the service is live
  19:
    criteria: Build a service that can be iterated on a frequent basis and make sure resources are in place to do so
    prompts: |
      * How has the service being iterated frequently during the beta and how will this continue after the service is live?
      * Who in the team was responsible for user research, usability testing and identifying actionable data insights during the beta, and who will be responsible after the service is live?
      * Do you have access to the data sets you need e.g. analytics, call centre data?
      * What have you thrown away or changed during the beta?

    evidence: |
      Service Manager able to:

        - show that the service has being iterated on a frequent basis during the beta, based on user research, usability testing and analytics, and explain how this will continue after the service is live
        - explain who in the team was responsible for user research, usability testing and identifying actionable data insights during the beta, and who will be responsible after the service is live
        - show they have access to the data sets they need e.g. analytics, call centre data
        - explain what they have thrown away or changed during the beta

Design:
  9:
    criteria: Create a service that is simple and intuitive enough that users succeed first time, unaided
    prompts: |
      * What evidence can you provide that users are, in the majority of cases, succeeding first time?
      * Are less digitally minded and non subject area experts able to use the beta service?
      * How were the design and content decisions made for the beta?
      * Has the service been tested for accessibility?
      * Are you demonstrating the happy path - can you show us other paths in the service?

    evidence: |
      Service Manager able to:

        - show the majority of users of the beta service are succeeding first time
        - show video of a less digitally minded user and non subject area experts succeeding to use the beta service
        - explain how the design and content decisions for the beta were made, and relate back to user research, usability testing and analytics
        - show the beta service is accessible
        - explain other paths in the beta service and demonstrate that they work
  12:
    criteria: Integrate the service with any non-digital sections required for legal reasons
    prompts: |
      * Does the service have any non-digital steps and if so, why?
      * What has been done to ensure that any non-digital steps work seamlessly with the digital service?
      * Have you tested the non-digital steps with users during the beta?
      * How are you planning to overcome any policy constraints and legislative barriers that are preventing the service from been an end to end digital service?

    evidence: |
      Service Manager able to:

        - explain why any non-digital steps are required for the live service
        - how any non-digital steps seamlessly link to the digital service and how they have tested them with users during the beta
        - explain how they intend to overcome the policy constraints or legislative barriers that are preventing the service from been an end to end digital service
        - demonstrate non-digital steps have been user tested
  13:
    criteria: Build a service consistent with the user experience of the rest of GOV.UK by using the design patterns and style guide.
    prompts: |
      * Has a designer and content designer been involved during the beta?
      * Will there be a designer and content designer in the team or available to the team after the service is live?
      * Have you used the GOV.UK design patterns and front end tool kit during the beta?
      * Do you have a front end developer in place for after the service is live?
      * Have you used the GDS style guide during the beta?
      * Is the service responsive? Can you show us it works on mobile?
      * Do the headers and footers match the GOV.UK style?

    evidence: |
      Service Manager able to:

        - explain how the service has used the GOV.UK design patterns, front end tool kit and GDS style guide in the beta.
        - explain what design, content design and front end developer support will be available to the team after the service is live
        - show the service is responsive and works on mobile
        - show that the headers and footers match the GOV.UK style

Assisted Digital and Channel Shift:
  10:
    criteria: Put appropriate assisted digital support in place that’s aimed towards those who genuinely need it
    prompts: |
      * How did you test and iterate your assisted digital support in beta, in response to user research and performance measurement?
      * Can you explain how the AD support you are putting in place meets user needs?
      * How many transactions are you expecting and how much does your AD support cost?
      * How will you measure the performance of your AD support when live?
      * How will you iterate your AD support in response to user testing and feedback when the service is live?

    evidence: |
      Service Manager able to:

        - Explain how you tested and iterated your AD support during beta, including identity assurance
        - Explain how you measured performance and what the results were, including user satisfaction with the AD support and confidence that the AD support provider was trustworthy
        - Give examples of positive feedback from users and experts
        - Explain how the design of the following elements of each channel of the service’s AD support are meeting user needs:
          * channels and providers
          * user awareness
          * user journeys (including identity assurance and needs assessment)
          * availability of and wait times for support
          * availability of appropriate technology
          * approach to users’ privacy
          * consistency with similar government transactions
          * approach to digital inclusion
          * ability to forecast and respond to changing demand
        - Show that costs per minute are in line with estimates, per channel
        - Give details of expected number of transactions per year, per channel
        - Explain how you will measure the performance of your AD support, including user satisfaction and decrease in the number of AD users
        - Explain your plans to enable iteration of your AD support once the service has gone live

  11:
    criteria: Plan (with GDS) for the phasing out of any existing alternative channels, where appropriate
    prompts: |
      * How did you identify users’ perceived risks and how do you know it’s working?
      * How have you used analytics and user research to reduce drop-out rates for your digital service? 
      * How do you know significant channel shift is happening? 
      * What is your plan for phasing out non-digital channels?
      * What is your plan for increasing digital take up to 82% over the next 5 years?
      * Tell us about your evidence base to support these plans?

    evidence: |
      Service Manager able to:

        - explain their plan for moving users to the digital service including year by year targets for increasing digital take up for the next 5 years
        - explain their plan to phase out non-digital channels as digital take up increases over the next 5 years
        - explain the evidence base behind their plans for increasing digital take up and phasing out non-digital channels
        - Show and explain usage volumes (and trends) per channel

Analysis and Benchmarking:
  8:
    criteria: Analyse the prototype service’s success, and translate user feedback into features and tasks for the next phase of development
    prompts: |
      * What testing did you do during the beta?
      * What did you plan to test?
      * How did you test the beta with end users?
      * What did you learn?
      * What did you change?
      * What didn't you change and why?
      * Have you agreed start and end pages with GOV.UK

    evidence: |
      Service Manager able to:

        - show videos of usability testing during beta
        - talk clearly about user research reports, indicating where a service required remediation
        - talk through substantial iteration in the design and content of the service during the beta
        - have agreed start and end pages with GOV.UK and these are optimised
  18:
    criteria: Use analytics tools that collect performance data
    prompts: |
      * What analytics tool have you installed?
      * Has the SIRO signed this off?
      * Have you anonymised the user IP address?
      * Have you opted out of data sharing with 3rd parties?
      * In addition to the 4 KPI's, what other KPI's are you tracking?
      * Are you able to track progression through your transaction so you can identify areas of poor performance?
      * Have you tracked exit paths?
      * Have you tracked outbound links?
      * Who in the team is responsible for identifying actionable data insights?
      * Have you built a conversion funnel and who will work on it?
      * Can you show us your dashboard with KPI's and any additional metrics?
    evidence: |
      Service Manager able to:

        - explain what analytics tool they have used during the beta and intend to use for the live service
        - show that the SIRO has signed it off for the live service
        - explain for their chosen analytics tool that they have anonymised the user IP address and have opted out of sharing data with 3rd parties for the live service
        - explain what additional KPI's they have tracked during the beta and will track for the live service
        - explain how they will track progression through the transaction so they can identify areas of poor performance in the live service
        - explain how they will be tracking exit paths and outbound links for the live service
        - explain who in the team was responsible for identifying actionable data insights during the beta and who will be responsible after the service goes live
        - explain how they have built a funnel and who will be working on it after the service is live
        - show their dashboard and explain what KPI's and additional metrics they are measuring
  7:
    criteria: Establish performance benchmarks, in consultation with GDS, using the 4 key performance indicators (KPIs) defined in the manual, against which the service will be measured
    prompts: |
      * Can you show us how you are reporting data on the 4 KPI's on your dashboard?
      * How are you measuring the performance of the old service to provide a baseline for the new service?
    evidence: |
      Service Manager able to:

        - explain how they are reporting the 4 KPI's on their dashboard
        - explain how they currently measure the performance of the old service to provide a baseline for the new service
  group:
    points: 
      21: Establish a benchmark for user satisfaction across the digital and assisted digital service. Report performance data on the Performance Platform.
      22: Establish a benchmark for completion rates across the digital and assisted digital service. Report performance data on the Performance Platform.
      23: Make a plan (with supporting evidence) to achieve a low cost per transaction across the digital and assisted digital service. Report performance data on the Performance Platform.
      24: Make a plan (with supporting evidence) to achieve a high digital take-up and assisted digital support for users who really need it. Report performance data on the Performance Platform.
    prompts: |
      * Can you show us how you are reporting user satisfaction on your dashboard?
      * How are you planning to increase user satisfaction after the service goes live?
      * How are you measuring the performance of the old service to provide a baseline for the new service?
      * Can you show us how you are reporting completion rates on your dashboard?
      * How are you planning to increase completion rates after the service goes live?
      * Can you show us how you are reporting cost per transaction on your dashboard?
      * How are you planning to achieve a low cost per transaction after the service goes live?
      * Can you show us how you are reporting digital take up on your dashboard, and does it demonstrate channel shift is taking place?
      * What are your plans and (year on year) targets for increasing digital uptake after the service goes live?
      * How are you measuring the performance of the old service to provide a baseline for the new service?
      * Are your delivery partners and other stakeholders promoting your digital service?
    
    evidence: |
      Service Manager able to:

        - explain and show how they are reporting user satisfaction on their dashboard
        - explain how they are planning to increase user satisfaction after the service goes live
        - explain who will monitor user satisfaction after the service is live
        - explain how they currently measure the performance of the old service to provide a baseline for the new service
        - explain and show how they are reporting completion rates on their dashboard
        - explain how they are planning to increase completion rates after the service goes live
        - explain who will monitor completion rates after the service is live
        - explain how they are reporting cost per transaction on their dashboard
        - explain how they are planning to achieve a low cost per transaction after the service goes live
        - explain who will monitor cost per transaction after the service is live
        - explain how they are reporting digital take up on their dashboard, and explain how it measures channel shift
        - explain how they are planning to increase digital uptake after the service goes live
        - realistic planning to increase digital uptake (thereby reducing reliance on assisted digital) after the service goes live
        - explain who will monitor digital take up after the service is live
        - that delivery partners and other stakeholders are actively promoting/supporting digital delivery

Testing with the Minister:
  26:
    criteria: Test the service from beginning to end with the minister responsible for it
    prompts: |
      * Has the minister responsible for the service tested it?
    evidence: |
      Service Manager able to:

        - show a video of the responsible minister testing the service or provide written confirmation signed by the responsible minister to confirm they have tested the service



