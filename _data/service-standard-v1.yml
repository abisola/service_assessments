criteria:
  1: Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for digital and assisted digital service design.
  2: Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.
  3: Evaluate what user data and information the service will be providing or storing, and address the security level, legal responsibilities, and risks associated with the service (consulting with experts where appropriate).
  4: Evaluate the privacy risks to make sure that personal data collection requirements are appropriate.
  5: Evaluate what tools and systems will be used to build, host, operate and measure a service, and how to procure them.
  6: Build the service using the agile, iterative and user-centred methods set out in the manual.
  7: Establish performance benchmarks, in consultation with GDS, using the 4 key performance indicators (KPIs) defined in the manual, against which the service will be measured.
  8: Analyse the prototype service’s success, and translate user feedback into features and tasks for the next phase of development.
  9: Create a service that is simple and intuitive enough that users succeed first time, unaided.
  10: Put appropriate assisted digital support in place that’s aimed towards those who genuinely need it.
  11: Plan (with GDS) for the phasing out of any existing alternative channels, where appropriate.
  12: Integrate the service with any non-digital sections required for legal reasons.
  13: Build a service consistent with the user experience of the rest of GOV.UK by using the design patterns and style guide.
  14: Make sure that you have the capacity and technical flexibility to update and improve the service on a very frequent basis.
  15: Make all new source code open and reuseable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code).
  16: Use open standards and common government platforms (eg identity assurance) where available.
  17: Be able to test the end-to-end service in an environment identical to that of the live version on all common browsers and devices. Use dummy accounts and a representative sample of users.
  18: Use analytics tools that collect performance data.
  19: Build a service that can be iterated on a frequent basis and make sure resources are in place to do so.
  20: Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users.
  21: Establish a benchmark for user satisfaction across the digital and assisted digital service. Report performance data on the Performance Platform.
  22: Establish a benchmark for completion rates across the digital and assisted digital service. Report performance data on the Performance Platform.
  23: Make a plan (with supporting evidence) to achieve a low cost per transaction across the digital and assisted digital service. Report performance data on the Performance Platform.
  24: Make a plan (with supporting evidence) to achieve a high digital take-up and assisted digital support for users who really need it. Report performance data on the Performance Platform."
  25: Make a plan for the event of the service being taken temporarily offline.
  26: Test the service from beginning to end with the minister responsible for it.
