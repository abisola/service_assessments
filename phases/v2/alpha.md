---
layout: default
title: Alpha
version: v2
redirect_from:
  - /phases/v2/
  - /phases/

---

# Alpha Assessment

**Note:** The 18-point service standard came into use on 1 June 2015.

## Point 1: Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for the design of the service.

### Additional Guidance

In the alpha phase, the main objective is to establish whether the team has a good understanding of user needs that has come from observing and engaging with end users, that they understand what users are trying to do when they engage with the current service (the user context, whether currently digital or not) and they they understand the user needs - not just functional requirements - that the service will have to achieve in order to be successful. Responses should cover both the digital and assisted digital support.

Further, the team need to demonstrate how they have explored design options that will best meet these needs, what concepts they have discarded and why they believe their final prototype has the potential to meet users needs effectively.

The assessors are less interested in the quantity of user research that has been undertaken, but rather in the quality and coverage - was it with the right people, was it the right kind of user research done in the right way/place, has it effectively created a user-centred, empathetic view of the project for the team.

The assessors will be interested in how the team has used mixed methods / sources of data to corroborate key findings (eg mixing analytics data with qualitative research findings).

When doing user research for assisted digital, ensure that research is done specifically with (potential) users of this particular service who have the lowest level of digital skills. Recruitment and research with this audience will need to be done using offline methods.

### Prompts

#### Questions about user needs:

* Who are the users?
* What have you done to understand your users’ needs?
* Tell us about what users are trying to do when they encounter your service?
* What are the needs that they have when they use this service?
* How do they meet those needs now?
* What are the pain points?
* Which users have the most challenging needs?
* How have you been learning more about these challenging user needs?
* What are the particular design challenges for this service with this audience?
* Tell us about what you’ve learned about the particular needs of people who are less confident online or not online?

#### Questions about the alpha prototype and user needs:

* What design options did you explore?
* Why did you discard some?
* Why did you select this one to move forward?
* In the prototype that you have selected can you show you user research has influenced the design of particular elements? (showing iteration over time)
* Which parts of the service have proved most problematic? What solutions have you been exploring to solve these problems?
* What design options are you considering for your assisted digital support and why?

### Evidence

It is very useful to include your user researcher in the team presenting at the assessment to answer assessor's questions.

The user researcher and/or service manager should be able to answer questions from the assessment panel by showing and referring to some or all of these artefacts of user research (for the onscreen service and assisted digital support), which include:

* User research output of discovery that describes how users (including assisted digital users) are currently meeting the needs that this service will meet, (e.g. a customer journey map or user needs map), key pain points in the current journey and description of the user research that has informed this output.
* Stories of people you have met, persona, profiles or some other way of telling the stories of the users (including assisted digital users) who will be using this service in the future.
* The user needs you have identified for this service, including any specific needs of assisted digital users.
* Any key insights you have gained from the research that describes significant service design challenges for this project to overcome.


## Point 2: Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users to improve the service.

### Additional Guidance

The main objective is to ensure that you have someone on the team who is dedicated to doing the user research, that there are plans to continue doing user research, and that there is evidence that outcomes from the user research will be fed into the ongoing development/design of the service. Responses should cover both digital and assisted digital support.

When doing user research for assisted digital, ensure that research is done specifically with (potential) users of this particular service who have the lowest level of digital skills. Recruitment and research with this audience will need to be done using offline methods.

### Prompts
* Are the resources in place to do regular user research and usability testing?
* Who in the team is doing user research and usability testing?
* How often are you doing user research and usability testing?
* How do the results feed into the design of the service?
* What is the user research plan for the next stage and are there resources for user research and usability testing?

### Evidence

User Researcher and/or Service manager able to:

*  explain who is doing user research and usability testing and how it is being resourced.
*  talk through the research plan for the next stage of the project.
*  explain how the results from user research and usability testing are incorporated into the design of the service.


## Point 3: Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.

### Prompts
* Can you talk us through your team for alpha?
* How has the service manager been empowered to make decisions during alpha?
* Is the service manager the single responsible person with the power and knowledge to make decisions to improve the service day-to-day during alpha?
* Are there any gaps in the team and how are you addressing these?
* Is there a separation of key roles?
* How will your assisted digital support be sustainably funded and free to the user?

### Evidence

Service Manager able to:

*  clearly explain the structure of the team for alpha (the following should be either in the team or available to the team depending on the scale of the service - service manager, product manager, delivery manager, tech architect and lead, assisted digital lead, designer, user researcher, developers, content designer, web ops, performance analyst).
*  explain how they are empowered to make decisions during alpha.
*  explain where they have gaps in the team and explain how they will address them.
*  show that there is a separation of key roles (i.e. the same person is not performing multiple roles within the service).
*  there is at least one user researcher working at least 3 days per week.
*  explain how you plan to ensure assisted digital support is sustainably funded (including support not delivered by government) and free to the user.

## Point 4: Build the service using the agile, iterative and user-centred methods set out in the manual.

### Prompts

* Talk us through how you are working in an agile way.
* What tools and techniques are you using to enable this way of working?
* How are you reviewing and iterating your processes?
* How are you able to adapt your processes to be responsive and iterate?
* How are you communicating within the team?
* Can you give an example of how you have responded to user research and usability testing?
* How are you governing the service?

### Evidence

Service Manager able to

* Clearly explain how the service is working in an agile way, using agile tools and techniques.
* Explain how the service has reviewed and iterated their processes to be responsive.
* Explain how the team are using agile tools and techniques to communicate within the team.
* Give an example of how the service has responded to user research and usability testing.
* Show that governance is proportional, not imposed, is based on clear and measurable goals, “go and see” rather than “wait and hear”, a clear focus on managing change and risk in real time rather than at arbitrary points, human centred not process centred.

## Point 5: Build a service that can be iterated and improved on a frequent basis and make sure that you have the capacity, resources and technical flexibility to do so.

### Prompts

* What have you built and why?
* What is the cycle time of a story?
* How are you analysing and reacting to your user research?
* How have you derisked any technical challenges?

### Evidence

Service Manager able to:

* Explain what they have built in alpha and why.
* Explain the cycle time of a story during alpha.
* Explain how they are analysing and reacting to user research.
* Explain how they are derisking any technical challenges.

## Point 6: Evaluate what tools and systems will be used to build, host, operate and measure the service, and how to procure them.

### Prompts

Describe the languages, frameworks, and other technical choices you've made in alpha, and how those will affect the decisions you make in beta.
Describe the development toolchain that you would like to select for beta and why.

### Evidence

Service Manager able to:

* Describe the languages, frameworks, and other technical choices they’ve made in alpha, and how this will affect the decisions they make in beta.
* Describe the development toolchain they would like to select for beta and why.


## Point 7: Evaluate what user data and information the digital service will be providing or storing, and address the security level, legal responsibilities, privacy issues and risks associated with the service (consulting with experts where appropriate).

### Prompts

Describe the perceived threats to your service and how you are designing the prototype to mitigate them?

What fraud vectors exist and what controls are you prototyping?


### Evidence

Service Manager able to:

* Describe the perceived threats to the service and explain how they are designing the prototype to mitigate them.
* Explain what fraud vectors exist and what controls they are prototyping.

## Point 8: Make all new source code open and reusable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code).

### Prompts

* Talk us through your plan for making all new source code open and reusable?
* Do you own the intellectual property?
* Will I be able to reuse your code in another department?

### Evidence

Service Manager able to:

* Explain their plan for making all new source code open and reusable.
* Confirm that they own the intellectual property.
* Explain how a team in another department can reuse their code.

## Point 9: Use open standards and common government platforms where available.

### Prompts

* Are you locking yourself into any proprietary solutions where an open standard is available?
* Describe which common platforms you have identified that your intended system could use?
* Describe any common user needs you have identified and how are you going to address them in a consistent manner with the rest of government?

### Evidence

Service Manager able to:

* Explain how they are avoiding locking themselves into any proprietary solutions where an open standard is available.
* Describe what common platforms they have identified that their intended system could use.
* Describe any common user needs they have identified and how how they are going to address them in a consistent manner with the rest of government.

## Point 10: Be able to test the end-to-end service in an environment identical to that of the live version, including on all common browsers and devices, and using dummy accounts and a representative sample of users.

### Prompts

* What environments do you have?
* What are the common devices and browsers for users of your service?
* How have you designed the system around that?

### Evidence

Service Manager able to:

* Explain what environments they have.
* Explain what the common devices and browsers are for users of their service.
* Explain how they have designed the system around the common devices and browsers for users of their service.

## Point 11: Make a plan for the event of the digital service being taken temporarily offline.

### Prompts

* Explain the impact upon the user of the proposed service being unavailable?

### Evidence

Service Manager able to:

* Explain the impact upon the user of the proposed service being unavailable.

## Point 12: Create a service that is simple and intuitive enough that users succeed first time.

### Prompts

* How did you collect evidence that users of the alpha service are, in the majority of cases, succeeding first time?
* What prototype testing did you do during alpha?
* What did you plan to test?
* How did you test the prototype with end users?
* What did you learn?
* What did you change?
* What didn't you change and why?
* How many other versions of the prototype did you try?
* Why did you choose this version?

### Evidence

Service Manager able to:

* Explain how they collected evidence that users of the alpha service are, in the majority of cases, succeeding first time.
* Explain the service in a simple way to the panel.
* Talk through substantial iteration in the design and content of the service during alpha.
* Explain how prototype works as end-to-end user journey for all user needs, including those with assissted digital needs.

## Point 13: Build a service consistent with the user experience of the rest of GOV.UK including using the design patterns and style guide.

### Prompts

* Has a designer and content designer been involved during alpha?
* Have you used the GOV.UK design patterns and front end tool kit during alpha?
* Do you have a front end developer in place?
* Have you used the GDS style guide during alpha?

### Evidence

Service Manager able to:

* Explain how the service has used the GOV.UK design patterns, front-end tool kit and GDS style guide during alpha.
* Explain what design, content design and front-end developer support was available to the team during alpha.

## Point 14: Encourage all users to use the digital service (with assisted digital support if required), alongside an appropriate plan to phase out non-digital channels/services.

### Prompts

* What is your plan for increasing digital take up?
* What other channels is the service currently delivered through?
* Do you collect data for these channels?
* What proportion of your users currently use and complete the transaction per channel?
* Which organisations/groups help your users with the existing digital or non-digital services?
* How has the digital service been designed to give it clear advantages over the other channels?

### Evidence
Service Manager able to:

* Explain their plan for increasing digital take up.
* Explain what other channels the service is delivered through.
* Explain what data they collect on their other channels.
* Explain how they collect analytical data on service usage for each channel.
* Explain which organisations/groups help your user with the existing digital or non-digital services.
* Show customer insight from: research with real users, user demographics, attitudes, behaviours and channel preferences, and customer journey maps.
* Explain how each channel meets different users’ needs.

## Point 15: Use tools for analysis that collect performance data. Use this data to analyse the success of the service and to translate this into features and tasks for the next phase of development.

### Prompts

* What data and analysis have you done during discovery from existing or legacy systems or the wider digital landscape?
* What analysis have you made on the alpha?
* Who in the team is responsible for analysis during the alpha, including for assisted digital support?

### Evidence

Service Manager able to:

* Explain what data sources and analysis were undertaken in discovery.
* Explain how the shape of the service has influenced the choice of metrics, data points and data sources.
* Explain who in the team is responsible for identifying actionable data insights during alpha, including assisted digital support.
* Talk clearly about evidence from qualitative and quantitative data, what they learned from these sources and what changes to user needs/improvements they identified.
* Talk through how these were prioritised and what features were changed or implemented.

## Point 16: Identify performance indicators for the service, including the 4 mandatory key performance indicators (KPIs) defined in the manual. Establish a benchmark for each metric and make a plan to enable improvements.

### Prompts

* Have you identified the behaviour, characteristics and dependencies of the new service and the factors that influence the choice of metrics and data sources for those metrics?
* How are you measuring the performance of the existing service (if applicable) across digital, non-digital and assisted digital channels?
* Have you identified which new or existing (if applicable) measures will form the basis of the new service baseline?
* Have you identified (in addition to the 4 mandatory KPI's) other measures for the new service and where the data will be provided from?
* Have you investigated how the service is expected to perform in comparison with similar services?

### Evidence

Service Manager able to:

* Demonstrate how they currently measure the performance of the existing service (if applicable) and which measures will provide a baseline for the new service and why they have been chosen.
* Explain measures for the new service, including dependent transactions (e.g. authentication, search etc), and why they have been chosen.
* Identify where the data for the metrics will come from.
* Explain their plan and scope for measuring cost per transaction (or equivalent*).
* Explain how user satisfaction data will captured, and the choice of benchmark.
* Explain how completion rate (or equivalent*) data will be captured, and the choice of benchmark, including the selection of start and end point and eligibility and transaction stages.
* Explain how usage volumes for the existing (if applicable) and new services (across channels) are or will be measured and how usage trends and insights drawn from similar services are informing their digital take-up plans.
* Explain their plan or benchmark for each additional metric identified for the new service.
* Demonstrate they have registered the service with the performance platform and validated that the platform can support the metrics the service dashboard should present.

<aside>
* For non-transactional user journeys
</aside>


## Point 17: Report performance data on the Performance Platform.

### Prompts:

* Have you registered your service with the Performance Platform?

### Evidence

Service Manager able to:

* Demonstrate they have registered the service with the Performance Platform and validated that the platform can support the metrics the service dashboard should present.

## Point 18: Test the service from beginning to end with the minister responsible for it.

### Prompts

How are you planning to test the service with the minister responsible for it before the service moves into live?

### Evidence

Service Manager able to:

* Confirm the minister responsible for the service will test it before the service moves into live.

