---
layout: default
title: Beta
version: v2
---

## Point 1: Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for the design of the service.

### Additional Guidance

In the beta phase, the main purpose is to establish that the team has continued to build the service in a way that meets user needs, and this has been done in a way that makes the service easy for all users. We are particularly looking to see that findings from the user research are reflected in the design of the service as it progresses through this phase. Responses should cover both digital and assisted digital support.

When doing user research for assisted digital, ensure that research is done specifically with (potential) users of this particular service who have the lowest level of digital skills. Recruitment and research with this audience will need to be done using offline methods.

### Prompts

#### Questions about user needs:

* Who are the users?
* What have you done to understand your users’ needs?
* Tell us about what users are trying to do when they encounter your service?
* What are the needs that they have when they use this service?
* How do they meet those needs now?
* What are the pain points?
* Which users have the most challenging needs?
* How have you been learning more about these challenging user needs?
* What are the particular design challenges for this service with this audience?
* Tell us about what you’ve learned about the particular needs of people who are less confident online or not online? What did you do to discover this?"

#### Questions about usability:

* How many rounds of usability testing have you done so far?
* Who have you done usability testing with?
* What were the tasks you set for participants and what materials did you supply to help them complete the task (if relevant).
* GIve us some specific examples of how aspects of the interface design has changed in each iteration in response to usability testing? (Show us your build/measure/learn cycles, what hypotheses did you test? what happened? what did you do?)
* Can most people get through the service end-to-end without assistance?
* Which users have the most challenging needs?
* How have you been learning more about these challenging user needs?
* Have you started testing methods for supporting people who do need assistance to get through the service?
* Who have you done this testing with and what were your findings?
* Has your testing included the supporting content and proposed start page for the service?
* What issues have you not yet resolved? What are your hypotheses around how you might solve those issues? How will you test that?
* What issues are you unable to test/resolve in beta thus far? How are you going to handle that as you move into public beta?
* Have you tested whether the name of the service makes sense to your users?

### Evidence

It is very useful to include your user researcher in the team presenting at the assessment to answer assessor's questions.

The user researcher and/or service manager should be able to answer questions from the assessment panel by showing and referring to some or all of these artefacts of user research (for the onscreen service and assisted digital support), which include:

* User research output of discovery that describes how users (including assisted digital users) are currently meeting the needs that this service will meet, (e.g. a customer journey map or user needs map), key pain points in the current journey and description of the user research that has informed this output.
* Stories of people you have met, persona, profiles or some other way of telling the stories of the users (including assisted digital users) who will be using this service in the future.
* The user needs you have identified for this service, including any specific needs of assisted digital users.
* Any key insights you have gained from the research that describes significant service design challenges for this project to overcome.
* Your research schedule for beta thus far (who you did research with, when and where, including assisted digital users).
* User research videos and accompanying user research analysis outputs for recent sprints.
* Examples showing how the design of various elements in the service have changed from iteration to iteration in response to user research.
* Findings from user research undertaken for assisted digital support, with potential assisted digital users of the specific service, with the lowest level of digital skills, confidence and access and through offline methods.

## Point 2: Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users to improve the service.

### Additional Guidance

The main objective is to ensure that you have someone on the team who is dedicated to doing the user research, that there are plans to continue doing user research, and that there is evidence that outcomes from the user research will be fed into the ongoing development/design of the service. Responses should cover both digital and assisted digital support.

When doing user research for assisted digital, ensure that research is done specifically with (potential) users of this particular service who have the lowest level of digital skills. Recruitment and research with this audience will need to be done using offline methods.

Accessibility testing with people who have particular access needs should be done throughout the service design process and not outsourced as a separate activity at the end of the design process.

### Prompts

Are the resources in place to do regular user research and usability testing?
Who in the team is doing user research and usability testing?
How often are you doing user research and usability testing?
Are you testing with a full range of end users, including those with low or no ability to use the digital service?
Are you doing regular usability testing with people who have particular access needs (accessibility testing)?
How is the analytics data feeding into the research plan for the service?
How do the results feed into the design of the service?
What is the user research plan for the next stage and are there resources for user research and usability testing?

### Evidence

User Researcher and/or Service manager able to:

* Explain who is doing user research and usability testing and how it is being resourced.
* Talk through the research plan for the next stage of the project.
* Talk through the plan to ensure the research covers the full range of end users including accessibility and assisted digital users.
* Explain how the results from user research and usability testing are incorporated into the design of the service.

## Point 3: Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.

### Prompts

* Can you talk us through the team that was in place during alpha and what the team looks like in beta?
* How was the service manager empowered to make decisions during alpha and is this the case in beta?
* Can you give us an example?
* Is the service manager the single responsible person with the power and knowledge to make decisions to improve the service day-to-day during beta?
* Were there any gaps in the team during alpha and how are you addressing these in beta?
* Are you using external people and if so, how are you approaching transferring skills and knowledge to the team during beta?
* Is there a separation of key roles?

### Evidence

Service Manager able to:

* Clearly explain the structure of the team during alpha and beta (the following should be either in the team or available to the team depending on the scale of the service - service manager, product manager, delivery manager, tech architect and lead, assisted digital lead, designer, user researcher, developers, content designer, web ops, product analyst).
* Show they were empowered to make decisions during alpha and are continuing to do so during beta.
* Show there are no gaps in the team or explain how they are addressing any gaps in beta.
* Explain how they will transfer knowledge from external people to the team during beta.
* Show that there is a separation of key roles (i.e. the same person is not performing multiple roles within the service).
* There is at least one user researcher working at least 3 days per week.

## Point 4: Build the service using the agile, iterative and user-centred methods set out in the manual.

### Prompts

* Talk us through how you have worked in an agile way during alpha and how you are doing so in beta?
* What tools and techniques have you used during alpha to enable this way of working?
* How are you reviewing and iterating your processes?
* How are you adapting your processes to be responsive and iterating them?
* How are you communicating within the team?
* Can you give an example from alpha of how you have responded to user research and usability testing?
* How are you governing the service?

### Evidence

Service Manager able to

* Clearly explain how the service has worked in an agile way during alpha and will continue to do so in beta, giving examples of using agile tools and techniques.
* Explain how the service has reviewed and iterated their processes to be responsive during alpha.
* Explain how the team has used agile tools and techniques to communicate within the team.
* Give an example of how the service has responded to user research and usability testing during alpha.
* Show that governance is proportional, not imposed, is based on clear and measurable goals, “go and see” rather than “wait and hear”, a clear focus on managing change and risk in real time rather than at arbitrary points, human centred not process centred.

## Point 5: Build a service that can be iterated and improved on a frequent basis and make sure that you have the capacity, resources and technical flexibility to do so.

### Prompts

* What have you built and why?
* Describe the lifecycle of a story from user research to production.
* What are you doing to support frequent deployments with minimal user impact?
* How long are you expecting the beta period to last and why?

### Evidence

Service Manager able to:

*  explain what they have built to this point and why.
*  describe clearly the lifecycle of a story from user research to production.
*  explain their deployment process and how they are able to support frequent deployments with minimal user impact.
*  explain how long they expect the beta period to last and the reasons behind it.

## Point 6: Evaluate what tools and systems will be used to build, host, operate and measure the service, and how to procure them.

### Prompts

* How are you managing the constraints that the selection of technology stack places on you?
* How are you managing the constraints that the selected development toolchain places on you?
* What have you bought and how have you ensured you are getting value for money?
* How will you know if the service is healthy?
* What support arrangements have you got in place during beta?
* What decision making have you outsourced?


### Evidence

Service Manager able to:

* Explain how they are managing the constraints that the selection of technology stack places on the service.
* Explain how they are managing the constraints that the selected development toolchain places on the service.
* Explain what they have bought and how they are ensuring they are getting value for money.
* Explain or demonstrate how they will know if the service is healthy.
* Explain the support arrangements they have in place during beta.
* Explain what decision making they have outsourced.

## Point 7: Evaluate what user data and information the digital service will be providing or storing, and address the security level, legal responsibilities, privacy issues and risks associated with the service (consulting with experts where appropriate).

### Prompts

* Describe your teams approach to security and risk management.
* Describe the threats to your service.
* What fraud vectors exist and what controls are you putting in place?
* Describe your interactions with the business and information risk teams, e.g. SIRO (Senior Information Risk Owner), IAO (Information Asset Owner), Data Guardians.
* Describe any outstanding legal concerns e.g. data protection or data sharing.
* Describe your cookie and privacy policy and how you arrived at it?

### Evidence

Service Manager able to:

* Describe their team’s approach to security and risk management.
* Describe the threats to their service.
* Explain what fraud vectors exist and what controls they are putting in place.
* Describe their interactions with the business and information risk teams e.g. SIRO (Senior Information Risk Owner), IAO (Information Asset Owner), Data Guardians.
* Describe any outstanding legal concerns e.g. data protection or data sharing.
* Present their cookie and privacy policy and explain how they arrived at it.

## Point 8: Make all new source code open and reusable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code).

### Prompts

* Describe how you are making new source code open and reuseable?
* What licences are you using to release code during beta?
* Do you own the intellectual property?
* Describe how a team in another department can reuse your code.
* What code from other teams/services are you using?

### Evidence

Service Manager able to:

* Explain how they are making new source code open and reusable.
* Show their code in an open internet source code repository.
* Explain what licences they are using to release code during beta.
* Confirm that they own the intellectual property.
* Explain how a team in another department can reuse their code.
* Explain what code from other teams/service they are using.

## Point 9: Use open standards and common government platforms where available.

### Prompts

* Are you locking yourself into any proprietary solutions where an open standard is available?
* What does the system output to the users and in what format?
* Describe your use of common government platforms
* Describe the integration mechanisms with any external systems
* What common user needs does your service meet and what are you reusing from across government to help meet that user need?
* What data do you hold and what is your open data responsibility?

### Evidence

Service Manager able to:

* Explain how they are avoiding locking themselves into any proprietary solutions where an open standard is available.
* Explain what the system outputs to users and in what format.
* Describe their use of common government platforms.
* Describe the integration mechanisms with any external systems.
* Explain any common user needs their service meets and what they are reusing from across government to help meet that user need.
* Explain what common data they hold and their open data responsibility.

## Point 10: Be able to test the end-to-end service in an environment identical to that of the live version, including on all common browsers and devices, and using dummy accounts and a representative sample of users.


### Prompts

* What environments do you have?
* How quickly and easily can you create a new environment?
* What data exists in your pre-production environments?
* How are you gaining confidence that your service will perform under expected loads?
* How are you checking that your system works on all the supported devices?

### Evidence

Service Manager able to:

* Explain what environments they have.
* Explain how quickly and easily they can create a new environment.
* Explain what data exists in their pre-production environments.
* Explain how they are gaining confidence that their service will perform under expected loads (including assisted digital routes).
* Describe testing environments, systems, and approaches for non-digital parts of the service (including assisted digital routes).
* Explain how they are checking that their system works on all the supported devices.
* Demonstrate their service in a live-like environment.

## Point 11: Make a plan for the event of the digital service being taken temporarily offline.

### Prompts

* Explain the impact upon the users of the beta service being unavailable for any length of time
* How are you selecting technology and platforms that meet your availability requirements?
* What is your data recovery strategy and have you tested it?
* Explain what things are most likely to take you offline and what mitigations you are considering
* What is your strategy for dealing with an incident? Who is responsible and what decisions can they make?

### Evidence

Service Manager able to:

* Explain the impact upon users of the beta service being unavailable for any length of time.
* Explain how they are selecting technology and platforms that meet their availability requirements.
* Explain their data recovery strategy and how they have tested it.
* Explain what things are most likely to take the service offline and what mitigations they are considering.

## Point 12: Create a service that is simple and intuitive enough that users succeed first time.

### Prompts

* What evidence can you provide that users are, in the majority of cases, succeeding first time?
* Were less digitally minded and non subject area experts able to use the beta service?
* How were the design and content decisions made in beta?
* Have you checked that the content used within the service aligns with the content published on relevant GOV.UK pages?
* Has the current version of the service been tested for accessibility?
* Are you demonstrating just the happy path - what thought has gone into other paths and can you show us they work?
* What prototype testing have you done so far?
* What did and do you plan to test?
* How did and do you test the prototype with end users?
* What have you learnt?
* What did you change?
* What didn't you change and why?
* How many other versions of the prototype did you try?
* Why did you choose this version?

#### Assisted digital support

* Which routes of assisted digital support will you be testing in beta?
* How do they meet user needs?
* What are your plans to test your assisted digital support during beta?
* Can you iterate your assisted digital support across all routes and providers, for the full end-to-end user journey?


### Evidence

Service Manager able to:

* Show the majority of users of the service are succeeding first time.
* Explain how the design and content decisions for the service were made, and relate back to user research, usability testing and analytics.
* Show the service is accessible.
* Explain other paths in the service and demonstrate that they work.
* Show videos of usability testing.
* Talk through substantial iteration in the design and content of the service.


#### Assisted digital support

Service Manager able to:

* Talk through how the assisted digital support has been designed to meet user needs, including routes and providers. If not providing all types (telephone and face by face, talk through and on behalf of), explain why.
* Explain the end-to-end user journeys for assisted digital support, including identity assurance (e.g. Verify) if required.
* Explain how you will test your assisted digital support in beta, with users with the lowest level of digital skills and access.
* Explain how you will test the end-to-end user journey for each route, including identity assurance (e.g. Verify) if required.
* Explain how you are able to iterate your assisted digital support across all routes and providers, for the full end-to-end journey?

## Point 13: Build a service consistent with the user experience of the rest of GOV.UK including using the design patterns and style guide.

### Prompts

* Has a designer and content designer been involved during the development so far?
* Is there a designer and content designer in the team or available to the team during beta?
* Have you used the GOV.UK design patterns and front-end tool kit during alpha and are you doing so during beta?
* Do you have a front-end developer in place for beta development?
* Have you used the GDS style guide during alpha and are you doing so during beta?
* Is the service responsive? Can you show us it works on mobile?
* Do the headers and footers match the GOV.UK style?

### Evidence
Service Manager able to:

* Explain how the service has used the GOV.UK design patterns, front-end tool kit and GDS style guide.
* Explain what design, content design and front-end developer support are available to the team during beta.
* Show the service is responsive and works on mobile.
* Show that the headers and footers match the GOV.UK style.


## Point 14: Encourage all users to use the digital service (with assisted digital support if required), alongside an appropriate plan to phase out non-digital channels/services.

### Prompts

* What is your plan for increasing digital take up during beta?
* Tell us about your evidence base to support these plans?
* How are you able to assess if users are shifting away from your non-digital channels to your digital one?
* How have you tested the effectiveness of your messaging with real users?
* What is your plan for engaging with other delivery channels (for your service) to promote digital take-up?

### Evidence

Service Manager able to:

* Explain how they plan to increase digital take up during beta.
* Explain the evidence base behind their plans for increasing digital take up.
* Demonstrate (at least) weekly analytics/metrics for usage volumes across channels.
* Demonstrate how your messaging has improved based on user insight and how it has performed based on analytics.
* Explain engagement across all delivery channels and planning for promoting digital take-up.

## Point 15: Use tools for analysis that collect performance data. Use this data to analyse the success of the service and to translate this into features and tasks for the next phase of development.

### Prompts

* What have you instrumented and why? Have you modelled user journeys, and are you able to track progression through your service so you can identify completions and areas of poor performance?
* Where appropriate, have you built a funnel, exit paths?
* What tools are you using to collect data?
* Has the SIRO signed these off?
* Where appropriate, have you anonymised the user IP address, have you opted out of data sharing with 3rd parties?
* What analysis has been carried out on the service so far and how has this impacted on the backlog?
* What is the ongoing roadmap for performance analysis, including performance of assisted digital support?
* Who in the team is responsible for identifying actionable data insights, including for assisted digital support?
* What is the next performance analysis user story?
* Have you started discussions with GOV.UK about start and end pages?

### Evidence

Service Manager able to:

* Explain what data sources and analysis have been undertaken in the alpha stage.
* Explain how the shape of the service has influenced the choice of metrics, data points and data sources.
* Explain the choice of analysis tools used in the beta (and alpha if appropriate).
* Show that appropriate information security and privacy issues have been addressed.
* Explain how they have modelled user journeys and will track progression through the service so they can identify completions and areas of poor performance in the beta.
* Talk clearly about evidence from qualitative and quantitative data, what they learned from these sources and what changes to user needs/improvements they identified.
* Talk through how these were prioritised and what features were changed or implemented.
* Talk about the ongoing roadmap for performance analysis, and explain who in the team is responsible for identifying actionable data insights during the beta, including for assisted digital support.
* Explain the next performance analysis user story.
* Show they have discussed and agreed start and end pages with GOV.UK and these are optimised.

## Point 16: Identify performance indicators for the service, including the 4 mandatory key performance indicators (KPIs) defined in the manual. Establish a benchmark for each metric and make a plan to enable improvements.

### Prompts

* Have you identified the behaviour, characteristics and dependencies of the new service and the factors that influence the choice of metrics and data sources for those metrics?
* Have these metrics changed since alpha review, and if so why?
* How are you measuring the performance of the existing (if applicable) and beta service across digital, non-digital and assisted-digital channels?
* How are you capturing user journey data for the new service during beta?
* How are you collecting cost per transaction (or equivalent*) data during beta?
* How are you collecting usage volume or digital take up data during beta?
* How are you collecting cost per transaction (or equivalent*) data during beta?
* How are you collecting other identified metrics data during beta?
* How are data analytics and user research being used to identify % of users who could channel shift and who will need assisted digital services?
* What are their plans for engaging with service users, delivery partners or other stakeholders during the beta phase?

### Evidence

Service Manager able to:

* Explain changes to the metrics collected since alpha and the reasons for change (if any).
* Show how they currently measure the performance of the existing service (if applicable) and which measures will provide a baseline for the new service.
* Show how they are collecting users journey data for the new service during beta (using a chosen analytics package).
* Show how they are collecting and calculating cost per transaction (or equivalent*) during beta.
* Show how they are collecting user satisfaction data during beta (GOV.UK 'done page' questionnaire or other).
* Show how they are collecting completion rate (or equivalent*) data during beta.
* Show how they are collecting other metrics data (including journey stage information) during beta.
* Explain how they plan to increase digital take up during beta.
* Explain how they are assessing potential for channel shift and level of assisted digital services required.
* Explain how they are engaging with beta users, delivery partners and other stakeholders to manage transition to live service.
* Explain how they will track migration from online to offline.
<aside>
  * for non-transactional user journeys
</aside>


## Point 17: Report performance data on the Performance Platform.

### Prompts

* Have you agreed what metrics are going on the performance platform?
* What data is being uploaded to the performance platform during beta?
* How is data being uploaded to the performance platform during beta?
* Is the performance dashboard available for use?

### Evidence

Service Manager able to:

* Show the beta dashboard with baseline data and explain the audience and use of the dashboard during beta.
* Show which metrics are being uploaded to the performance platform and how they are uploaded (manual, automatic).
* Show published performance dashboard.

## Point 18: Test the service from beginning to end with the minister responsible for it.

### Prompts

* How are you planning to test the service with the minister responsible for it before the service moves into live?

### Evidence

Service Manager able to

* Explain how they will test the service with the minister responsible for it before the service moves into live.
