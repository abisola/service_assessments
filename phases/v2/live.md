---
layout: default
title: Live
version: v2
---

# Live Assessment

**Note:** The 18-point service standard came into use on 1 June 2015.

## Point 1: Understand user needs. Research to develop a deep knowledge of who the service users are and what that means for the design of the service.

### Additional Guidance

In the beta phase the main purpose is to establish that the team has continued to build out the service in a way that meets user needs, that this has been done in a way that makes the service easy for all users. We are particularly looking to see that findings from the user research are reflected in the design of the service as it has progressed through this phase. Responses should cover both digital and assisted digital support.

When doing user research for assisted digital, ensure that research is done specifically with (potential) users of this particular service who have the lowest level of digital skills. Recruitment and research with this audience will need to be done using offline methods.

### Prompts

#### Questions about user needs:

* Tell us about what users are trying to do when they encounter your service?
* How do they meet their needs now?
* What are the pain points?
* What are the needs that they have when they use this service?
* What are the particular design challenges for this service with this audience?
* Tell us about what you’ve learned about the particular needs of people who are less confident online or not online?
* How did you learn this?

#### Questions about usability:
* How many rounds of usability testing have you done so far?
* Who have you done usability testing with?
* What were the tasks you set for participants and what materials did you supply to help them complete the task (if relevant).
* Give us some specific examples of how aspects of the interface design has changed in each iteration in response to usability testing? (Show us your build/measure/learn cycles, what hypotheses did you test? what happened? what did you do?)
* Can most people get through the service end-to-end without assistance?
* Have you started testing methods for supporting people who do need assistance to get through the service?
* Has your testing included the supporting content and proposed start page for the service?
* Who have you done this testing with and what were your findings?
* What issues have you not yet resolved? What are your hypotheses around how you might solve those issues? How will you test that?
* What issues are you unable to test/resolve in beta? How are you going to handle that as you move into live?
* Have you tested whether the name of the service makes sense to your users?

### Evidence

It is very useful to include your user researcher in the team presenting at the assessment to answer assessor's questions.

The user researcher and/or service manager should be able to answer questions from the assessment panel by showing and referring to some or all of these artefacts of user research (for the onscreen service and assisted digital support), which include:

* User research output of discovery that describes how users (including assisted digital users) are currently meeting the needs that this service will meet, (e.g. a customer journey map or user needs map), key pain points in the current journey and description of the user research that has informed this output.
* Stories of people you have met, persona, profiles or some other way of telling the stories of the users (including assisted digital users) who will be using this service in the future.
* The user needs you have identified for this service, including any specific needs of assisted digital users.
* Any key insights you have gained from the research that describes significant service design challenges for this project to overcome.
* Your research schedule for beta (who you did research with, when and where, including assisted digital users).
* User research videos and accompanying user research analysis outputs for recent sprints.
* Examples showing how the design of various elements in the service have changed from iteration to iteration in response to user research impact on user stories and product changes, explain how evidence from analytics has informed these experiments and outcomes.

## Point 2: Put a plan in place for ongoing user research and usability testing to continuously seek feedback from users to improve the service.

### Additional Guidance

The main objective is to ensure that you have someone on the team who is dedicated to doing the user research, that there are plans to continue doing user research, and that there is evidence that outcomes from the user research will be fed into the ongoing development/design of the service. Responses should cover both the digital and assisted digital support.

When doing user research for assisted digital, ensure that research is done specifically with (potential) users of this particular service who have the lowest level of digital skills. Recruitment and research with this audience will need to be done using offline methods.

Accessibility testing with people who have particular access needs should be done throughout the service design process and not outsourced as a separate activity at the end of the design process

### Prompts

* Are the resources in place to do regular user research and usability testing?
* Who in the team is doing user research and usability testing?
* How often are you doing user research and usability testing?
* Are you testing with a full range of end users, including those with low or no ability to use the digital service?
* Are you doing regular usability testing with people who have particular access needs (accessibility testing)?
* How is the analytics data feeding into the research plan for the service?
* How do the results feed into the design of the service?
* What is the user research plan for the next stage and are there resources for user research and usability testing?

### Prompts

* Are the resources in place to do regular user research and usability testing?
* Who in the team is doing user research and usability testing?
* How often are you doing user research and usability testing?
* Are you testing with a full range of end users, including those with low or no ability to use the digital service?
* Are you doing regular usability testing with people who have particular access needs (accessibility testing)?
* How is the analytics data feeding into the research plan for the service?
* How do the results feed into the design of the service?
* What is the user research plan for the next stage and are there resources for user research and usability testing?

## Point 3: Put in place a sustainable multidisciplinary team that can design, build and operate the service, led by a suitably skilled and senior service manager with decision-making responsibility.

### Prompts

* Can you talk us through the team that was in place during beta and what the team will look like after the service is live?
* Was the service manager empowered to make decisions during beta and will this remain be the case after the service goes live?
* Is the service manager the single responsible person with the power and knowledge to make decisions to improve the service day-to-day after the service is live?
* Were there any gaps in the team during beta and how are you addressing these for after the service goes live?
* Do you have plans to sustain the team after the service goes live?
* Is there an operating model in place for the live service?
* Are you using external people and if so, how are you approaching transferring skills and knowledge to the team after the service is live?
* Is there a separation of key roles?

### Evidence

Service Manager able to

* Clearly explain the structure of the team during beta and after the service goes live (the following should be either in the team or available to the team depending on the scale of the service - service manager, product manager, delivery manager, tech architect and lead, assisted digital lead, designer, user researcher, developers, content designer, web ops, product analyst)
* Show they were empowered to make decisions during beta and will continue to do so after the service is live
* Show there are no gaps in the team or explain how they will address any gaps
* Show that the team will be sustained to continuously improve the service after it goes live
* Show they fully understand the service
* Explain how they will transfer knowledge from external people to the team
* Show that there is a separation of key roles (i.e. the same person is not performing multiple roles within the service)
* There is at least one user researcher working at least 3 days per week

## Point 4: Build the service using the agile, iterative and user-centred methods set out in the manual.

### Prompts

* Talk us through how you have worked in an agile way during beta and how you will continue to do so after the service is live?
* What tools and techniques have you used during beta to enable this way of working?
* How have you reviewed and iterated your processes during beta?
* How have you adapted your processes to be responsive and iterated them during beta?
* How have you communicated within the team during beta?
* Can you give an example from beta of how you have responded to user research and usability testing?
* How are you governing the service?

### Evidence

Service Manager able to

* Clearly explain how the service has worked in an agile way during beta and will continue to do so after the service is live, giving examples of using agile tools and techniques.
* Explain how the service has reviewed and iterated their processes to be responsive during beta.
* Explain how the team has used agile tools and techniques to communicate within the team during beta.
* Give an example of how the service has responded to user research and usability testing during beta.
* Show that governance is proportional, not imposed, is based on clear and measurable goals, “go and see” rather than “wait and hear”, a clear focus on managing change and risk in real time rather than at arbitrary points, human centred not process centred.

## Point 5: Build a service that can be iterated and improved on a frequent basis and make sure that you have the capacity, resources and technical flexibility to do so.

### Prompts

* What have you built and why?
* How have you ensured zero downtime deployments or otherwise ensured that there is no user impact to doing a release?
* Describe the lifecycle of a story from user research to production.
* How will you staff/pay for the continued improvement of the service?

### Evidence

Service Manager able to:

* Explain what they have built to this point and why.
* Explain how they have ensured zero downtime deployments or otherwise ensured there is no user impact to doing a release.
* Describe clearly the lifecycle of a story from user research to production.
* Explain how they will staff/pay for continued improvement of the service.

## Point 6: Evaluate what tools and systems will be used to build, host, operate and measure the service, and how to procure them.

### Prompts

* Describe what tech stack changes you've made since beta and why.
* Describe what development toolchain changes you've made since beta and why.
* What are you doing to ensure that you are continuing to get value for money from the systems you selected and bought at beta?
* How will you know if the service is healthy?
* What support arrangements have you got in place for live?
* What decision making have you outsourced?

### Evidence

Service Manager able to:

* Describe what tech stack changes they’ve made during beta and why.
* Describe what development toolchain changes they’ve made during beta and why.
* Explain how they are ensuring they are continuing to get value for money from the systems they selected and bought at beta.
* Explain or demonstrate how they will know if the service is healthy.
* Explain the support arrangements they have in place for live.
* Explain what decision making they have outsourced."

## Point 7: Evaluate what user data and information the digital service will be providing or storing, and address the security level, legal responsibilities, privacy issues and risks associated with the service (consulting with experts where appropriate).

### Prompts

* Describe your teams approach to security and risk management.
* Describe your ongoing interactions with the business and information risk teams, e.g. SIRO (Senior Information Risk Owner), IAO (Information Asset Owner), Data Guardians.
* Describe any outstanding legal concerns e.g. data protection or data sharing.
* How are you keeping your understanding of the threats to your service up to date? Have the threats changed since beta?
* How are you keeping your cookie and privacy policy up to date?
* How are you staying aware of security updates to your systems and how quickly can you respond?

### Evidence

Service Manager able to:

* Describe their team’s approach to security and risk management.
* Describe their ongoing interactions with the business and information risk teams e.g. SIRO (Senior Information Risk Owner), IAO (Information Asset Owner), Data Guardians.
* Describe any outstanding legal concerns e.g. data protection or data sharing
* Explain how they are keeping their understanding of the threats to the service up to date, and explain how the threats have changed during beta
* Explain how they are keeping their cookie policy and privacy policy up to date
* Explain how they are staying aware of security updates to their systems and how quickly they can respond

## Point 8: Make all new source code open and reusable, and publish it under appropriate licences (or provide a convincing explanation as to why this cannot be done for specific subsets of the source code).

### Prompts

* Describe how you are making new source code open and reuseable?
* Describe how you accept contributions and comments on the code?
* How are you handling updates and bug fixes to the code?
* What licences are you using to release code?
* Do you own the intellectual property?
* What code have you not opened and why?

### Evidence

Service Manager able to:

* Explain how they are making new source code open and reusable
* Show their code in an open internet source code repository
* Describe how they accept contributions and comments on the code
* Explain how they are handling updates and bug fixes to the code
* Explain what licences they are using to release code
* Confirm that they own the intellectual property
* Explain what code they have not opened and why

## Point 9: Use open standards and common government platforms where available.

### Prompts

* Describe your use of common government platforms?
* Describe the integration mechanisms with any external systems.
* What common user needs does your service meet and what are you reusing from across government to help meet that user need?

### Evidence

Service Manager able to:

* Describe their use of common government platforms.
* Describe the integration mechanisms with any external systems.
* Explain any common user needs their service meets and what they are reusing from across government to help meet that user need.

## Point 10: Be able to test the end-to-end service in an environment identical to that of the live version, including on all common browsers and devices, and using dummy accounts and a representative sample of users.

### Prompts

* Describe any changes to the number and nature of the environments you’re using for testing.
* How frequently are you testing the system?
* How are you checking that your system works on all the supported devices?

### Evidence

Service Manager able to:

* Describe any changes to the number and nature of the environments they’re using for testing.
* Explain how frequently they are testing the system.
* Explain how they are checking that their system works on all the supported devices.
* Explain how they have ensured they have capacity for live, including non-digital parts of the service (e.g. assisted digital routes)

## Point 11: Make a plan for the event of the digital service being taken temporarily offline.

### Prompts

* Explain the impact upon the users of the service being unavailable for any length of time and how has that changed since beta?
* How are you ensuring your selected technology and platforms still meet your availability requirements?
* What is your data recovery strategy and how often are you testing it?
* Explain what things are most likely to take you offline and what mitigations you are considering?
* What is your strategy for dealing with an incident? Who is responsible and what decisions can they make?

### Evidence

Service Manager able to:

* Explain the impact upon the users of the service being unavailable for any length of time and how that has changed since beta.
* Explain how they are ensuring their selected technology and platforms still meet their availability requirements.
* Explain their data recovery strategy and how often they are testing it.
* Explain what things are most likely to take the service offline and what mitigations they are considering.
* Explain their strategy for dealing with an incident, including who is responsible and what decisions they can make.

## Point 12: Create a service that is simple and intuitive enough that users succeed first time.

### Prompts

* What evidence can you provide that users are, in the majority of cases, succeeding first time?
* Are less digitally minded and non subject area experts able to use the beta service?
* How were the design and content decisions made in beta?
* Have you checked that the content used within the service aligns with the content published on relevant GOV.UK pages?
* Has the service been tested for accessibility?
* Are you demonstrating the happy path - can you show us other paths in the service?
* What testing did you do during beta?
* What did you plan to test?
* How did you test the beta with end users?
* What did you learn?
* What did you change?
* What didn't you change and why?


#### Assisted digital support:

* What assisted digital support routes and providers did you test in beta?
* How did you test the support, with assisted digital users and for the full end-to-end user journey?
* What did you learn and how did you iterate the support, based on testing, metrics and user feedback, including changes to available routes?
* Is each route and provider flexible to cope with peaks in demand?"

### Evidence

Service Manager able to:

* Show the majority of users of the beta service are succeeding first time.
* Show video of a less digitally minded user and non subject area experts succeeding in using the beta service.
* Explain how the design and content decisions for beta were made, and relate back to user research, usability testing and analytics
* Show the beta service is accessible.
* Explain other paths in the beta service and demonstrate that they work.
* Show videos of usability testing during beta.
* Talk through substantial iteration in the design and content of the service during beta.

#### Assisted digital support

Service Manger able to:

* Explain what assisted digital support was tested in beta.
* Explain how the assisted digital support was tested in beta, including testing with users with the lowest level of digital skills and access, how those users were recruited and for the full end-to-end user journey, including identity assurance (e.g. Verify) if required.
* Talk through how the assisted digital support has been designed to meet user needs based on what you learnt from testing, metrics and user feedback, including:
  * routes and providers. If not providing all types (telephone and face by face, talk through and on behalf of), explain why.
  * user awareness of support
  * availability of and wait times for support
  * approach to digital inclusion
  * approach to users’ privacy (for face by face support)
  * availability of appropriate technology/equipment (for face by face support)
  * consistency with similar government transactions
* Explain how potential peaks in demand will be met, including users currently being helped by friends and family and regional demand.

## Point 13: Build a service consistent with the user experience of the rest of GOV.UK including using the design patterns and style guide.

### Prompts

* Has a designer and content designer been involved during the beta?
* Will there be a designer and content designer in the team or available to the team after the service is live?
* Have you used the GOV.UK design patterns and front end tool kit during the beta?
* Do you have a front end developer in place for after the service is live?
* Have you used the GDS style guide during the beta?
* Is the service responsive? Can you show us it works on mobile?
* Do the headers and footers match the GOV.UK style?

### Evidence

Service Manager able to:

* Explain how the service has used the GOV.UK design patterns, front-end tool kit and GDS style guide in beta.
* Explain what design, content design and front end developer support will be available to the team after the service is live.
* Show the service is responsive and works on mobile.
* Show that the headers and footers match the GOV.UK style.

## Point 14: Encourage all users to use the digital service (with assisted digital support if required), alongside an appropriate plan to phase out non-digital channels/services.

### Prompts

* How have you used analytics and user research to reduce dropout rates for your digital service?
* How do you know significant channel shift is happening?
* What is your plan for phasing out non-digital channels?
* What is your plan for increasing digital take up when live?
* Tell us about your evidence base to support these plans?

### Evidence

Service Manager able to

* Explain their plan for moving users to the digital service including year by year targets for increasing digital take up for the next 5 years.
* Explain their plan to phase out non-digital channels as digital take up increases over the next 5 years.
* Explain the evidence base behind their plans for increasing digital take up and phasing out non-digital channels.
* Show and explain usage volumes (and trends) per channel.

## Point 15: Use tools for analysis that collect performance data. Use this data to analyse the success of the service and to translate this into features and tasks for the next phase of development.

### Prompts

* What have you instrumented and why? Have you modelled user journeys, and are you able to track progression through your service so you can identify completions and areas of poor performance?
* Where appropriate, have you built a funnel, exit paths?
* What tools are you using to collect data?
* Has the SIRO signed these off?
* Where appropriate, have you anonymised the user IP address, have you opted out of data sharing with 3rd parties?
* What further analysis has been carried out on the service and how has this impacted on the backlog?
* What metrics and data sources have you chosen to measure your assisted digital support and why?
* What is the ongoing roadmap for performance analysis, including assisted digital support?
* Once live, who in the team is responsible for identifying actionable data insights, including assisted digital support?
* What is the next performance analysis user story?

### Evidence

Service Manager able to

* Explain what data sources and analysis have been undertaken in beta.
* Explain how the shape of the service has influenced the choice of metrics, data points and data sources.
* Explain the choice of analysis tools to be used on the live service.
* Show that appropriate information security and privacy issues have been addressed.
* Explain how they have modelled user journeys and will track progression through the service so they can identify completions and areas of poor performance in beta.
* Talk clearly about evidence from qualitative and quantitative data, what they learned from these sources and what changes to user needs/improvements they identified.
* Talk through how these were prioritised and what features were changed or implemented.
* Explain how assisted digital support will be measured and why.
* Talk about the ongoing roadmap for performance analysis, and explain who in the team is responsible for identifying actionable data insights in live, including for assisted digital support.
* Explain the next performance analysis user story.
* Demonstrate start and end pages with GOV.UK and that these are optimised.

## Point 16: Identify performance indicators for the service, including the 4 mandatory key performance indicators (KPIs) defined in the manual. Establish a benchmark for each metric and make a plan to enable improvements.

### Prompts

* Have you changed the behaviour, characteristics and dependencies of the new service and the factors that influence the choice of metrics and data sources for those metrics?
* Have these metrics changed since beta assessment and if so why?
* How are you measuring the performance of the existing (if applicable) and new service across digital, non-digital and assisted-digital channels?

### Evidence

Using the Performance Platform dashboard, the Service Manager is able to:

* Explain and show how they are reporting user satisfaction on their dashboard
* Explain how they are planning to increase user satisfaction after the service goes live
* Explain who will monitor user satisfaction after the service is live
* Explain and show how they are reporting completion rates (or equivalent*) on their dashboard
* Explain how they are planning to increase completion rates after the service goes live
* Explain who will monitor completion rates after the service is live
* Explain and show how they are reporting cost per transaction (or equivalent*) on their dashboard
* Explain how they are planning to achieve a low cost per transaction after the service goes live
* Explain who will monitor cost per transaction after the service is live
* Explain and show how they are reporting digital take up on their dashboard, and explain how it measures channel shift
* Explain how they are planning to increase digital uptake after the service goes live
* Explain realistic planning to increase digital uptake (thereby reducing reliance on assisted digital) after the service goes live
* Explain who will monitor digital take up after the service is live
* Explain and show other additional metrics supporting the service on their dashboard
* Explain how they are planning to use these metrics to improve the service
* Explain who will monitor these metrics
* Demonstrate that delivery partners and other stakeholders are actively promoting/supporting digital delivery
* Identify future metrics and how they will be used to improve the service
* Outline a timescale when these metrics will be available


<aside>* for non-transactional user journeys</aside>

## Point 17: Report performance data on the Performance Platform.

### Prompts

Demonstrate your service dashboard

### Evidence

Service manager able to:

* Show published performance platform dashboard, including metrics for the 4 KPIs, and other key metrics.
* Explain metrics and reason why they have been chosen.

## Point 18: Test the service from beginning to end with the minister responsible for it.

### Prompts

Has the minister responsible for the service tested it?

### Evidence

Service Manager able to:

* Show a video or photo of the responsible minister testing the service, or provide written confirmation signed by the responsible minister to confirm they have tested the service.

